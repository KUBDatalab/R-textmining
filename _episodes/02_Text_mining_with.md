---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 02_Text_mining_with.md in _episodes_rmd/
title: "Episode 2 tidytext, stopwords, and sentiment analysis"
teaching: 0
exercises: 0
questions:
- "What is a model?"
objectives:
- "First learning objective. (FIXME)"
keypoints:
- "First key point. Brief Answer to questions. (FIXME)"
---






## R Markdown
## Understanding our data

We have now successfully loaded in our dataset. Before we start preparing it for analysis, let us inspect the columns to see what the dataset contains

~~~
head(kina)
~~~
{: .language-r}



~~~
Error in head(kina): object 'kina' not found
~~~
{: .error}

We see that we have a lot of metadata, including the date of the speech, the start and end time of the speech, the discussed resolutions/law proposals and their classifications into subjects, as well as various personal information about the speaker. The last column is called `Text` and this contains the speech itself

## introduction to tidytext and tokenization
To analyze the speeches we need to make the text tidy. Tidy text refers to a dataset where each text has been split up into the individual words that make up the speech. [insert visualization of how words in one row are split into multiple rows]

Splitting texts, in our case speeches, into individual words is called tokenization [insert visualization of tokenization of a sentence]

Tokenization of text into individual words is necessary for text mining because it allows us to analyze the text closely and in detail, analyses which can later be visualized to understand the patterns of the text. Tokenization is language independent, as long as the language is written in an alphabet or syllabary that uses spaces between words. When tokenizating our text to make it tidy, the metadata that describe the whole speech are carried over to also describe the individual word. Thus we can split the text into individual words but still keep track of who said that word and when they did.

We use the tidytext library for tokenization

## Stopwords
In all natural language texts, frequent words that carry little meaning by themselves are distributed all across the text [insert visualization of sentence(s) where stopwords are highlighted].

The frequent low-meaning words need to be removed because they do not add anything to our understanding of the texts and are just noise

The tm library contains a list of stopwords for Danish, which we'll make into a tibble. We have to specify that the list of stopwrds that we want to call is the list for the Danish language. We also rename the tibble column that contains the stopwords. Note that stopword lists are also available for most major European languages


~~~
stopwords_dansk <- as_tibble(stopwords(kind = "danish"))
~~~
{: .language-r}



~~~
Error in as_tibble(stopwords(kind = "danish")): could not find function "as_tibble"
~~~
{: .error}



~~~
stopwords_dansk <- stopwords_dansk %>% 
  rename(word = value)
~~~
{: .language-r}



~~~
Error in stopwords_dansk %>% rename(word = value): could not find function "%>%"
~~~
{: .error}

## Sentiment analysis
Sentiment analysis is a method for measuring the sentiment of a text. To do this, it is necessary to have a list of words that have been assigned to a certain sentiment. This can be a simple assignation of words into positive and negative, it can be an assignation to one among a multitude of categories, and the word can have a value on a scale. In this course we will use the AFINN index for Danish, which assigns approximately 3500 words on a scale from +5 to -5. This will enable us to compare the overall sentiment of the various speeches. As a side note, AFINN index is also available in English. 

We need to download the AFINN Index from GitHub


~~~
download.file("https://raw.githubusercontent.com/swillerhansen/R-textmining/main/data/AFINN%20dansk.txt", "data/AFINN_dansk.txt", mode = "wb")
~~~
{: .language-r}

Now we read need to read the AFINN Index into a tibble and rename the columns


~~~
AFINN <- read_delim("data/AFINN_dansk.txt")
~~~
{: .language-r}



~~~
Error in read_delim("data/AFINN_dansk.txt"): could not find function "read_delim"
~~~
{: .error}



~~~
AFINN_dansk <- AFINN_dansk %>% 
  rename(
    word = X1,
    sentiment_value = X2)
~~~
{: .language-r}



~~~
Error in AFINN_dansk %>% rename(word = X1, sentiment_value = X2): could not find function "%>%"
~~~
{: .error}

## Bringing it all together: joins
We now have a method for tokenization of text, a stopword list to filter out stopwords, and a sentiment index to measure the sentiment of the parliament speeches. Now we need to bring it all together in the correct order, and we do this by using join-functions. The join functions from the tidyverse library allow tibbles to be joined together according to based on columns and rows that they have in common

There are fundamentally 2 types of joins:
Mutating joins (which add columns)
Filtering joins (which filter away rows)

Mutating joins work by adding new columns to the tibble. We will use left_join, which is the most common of the mutating joins
[insert Venn-diagram of left_join]
The left_join joins all AFINN sentiment values to those rows that contain a word that is in the AFINN Index and adds it as a new column to the tibble. In the new column, the rows that contain words that don't appear in the AFINN Index have NA in their cell

Filtering joins work by filtering away some rows in the tibble. We will use the anti_join, which removes those rows that contain a word that is also in the stopword list
[insert Venn-diagram of anti_join]

For more info on joins see https://r4ds.had.co.nz/relational-data.html


~~~
kina_tidy <- kina %>% 
  unnest_tokens(word, Text) %>% #tidytext tokenization
  anti_join(stopwords_dansk) %>% #stopwords in Danish
  left_join(AFINN_dansk, by = "word") #left join with AFINN Index in Danish
~~~
{: .language-r}



~~~
Error in kina %>% unnest_tokens(word, Text) %>% anti_join(stopwords_dansk) %>% : could not find function "%>%"
~~~
{: .error}


This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:


~~~
summary(cars)
~~~
{: .language-r}



~~~
     speed           dist       
 Min.   : 4.0   Min.   :  2.00  
 1st Qu.:12.0   1st Qu.: 26.00  
 Median :15.0   Median : 36.00  
 Mean   :15.4   Mean   : 42.98  
 3rd Qu.:19.0   3rd Qu.: 56.00  
 Max.   :25.0   Max.   :120.00  
~~~
{: .output}

## Including Plots

You can also embed plots, for example:

<img src="../fig/rmd-02-pressure-1.png" alt="plot of chunk pressure" width="612" style="display: block; margin: auto;" />

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
